{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsmX4CCZGTWD1G3yiF7rBn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heugyu/notebook/blob/master/GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTbsBWJu2kVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLznavmotY_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "LR = 0.001\n",
        "EPOCHS = 40\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7P4LbTNtyvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f93963bd-911c-4f89-f20d-43c6ed858d20"
      },
      "source": [
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.Field(sequential=False, batch_first=True)\n",
        "\n",
        "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "TEXT.build_vocab(trainset, min_freq=5)\n",
        "LABEL.build_vocab(trainset)\n",
        "\n",
        "trainset, valset = trainset.split(split_ratio=0.8)\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (trainset, valset, testset),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    repeat=False\n",
        ")\n",
        "\n",
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 2\n",
        "print(f'train : {len(train_iter)}  test : {len(test_iter)}  vocab : {vocab_size}  classes : {n_classes}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train : 313  test : 391  vocab : 46159  classes : 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aQqqMQ_wMWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "322183c9-f54c-4e2b-e05b-1b742dce0f74"
      },
      "source": [
        "class BasicGRU(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
        "        super(BasicGRU, self).__init__()\n",
        "        print('building basic gru model ...')\n",
        "        self.n_layers = n_layers\n",
        "        self.emded = nn.Embedding(n_vocab, embed_dim)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.gru = nn.GRU(\n",
        "            embed_dim,\n",
        "            self.hidden_dim,\n",
        "            num_layers=self.n_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emded(x)\n",
        "        h_0 = self._init_state(batch_size=x.size(0))\n",
        "        x, _ = self.gru(x, h_0)\n",
        "        h_t = x[:, -1, :]\n",
        "        self.dropout(h_t)\n",
        "        logit = self.out(h_t)\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, batch_size=1):\n",
        "        weight = next(self.parameters()).data\n",
        "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
        "\n",
        "def train(model, optimizer, train_iter):\n",
        "    model.train()\n",
        "    for b, batch in enumerate(train_iter):\n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        y.data.sub_(1)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logit = model(x)\n",
        "        loss = F.cross_entropy(logit, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, val_iter):\n",
        "    model.eval()\n",
        "    corrects, total_loss = 0, 0\n",
        "    for batch in val_iter:\n",
        "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
        "        y.data.sub_(1)\n",
        "        logit = model(x)\n",
        "        loss = F.cross_entropy(logit, y, reduction='sum')\n",
        "        total_loss += loss.item()\n",
        "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "    size = len(val_iter.dataset)\n",
        "    avg_loss = total_loss / size\n",
        "    avg_accuracy = 100.0 * corrects / size\n",
        "    return avg_loss, avg_accuracy\n",
        "\n",
        "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "best_val_loss = None\n",
        "for e in range(1, EPOCHS+1):\n",
        "    train(model, optimizer, train_iter)\n",
        "    val_loss, val_accuray = evaluate(model, val_iter)\n",
        "\n",
        "    print(f'에폭 : {e}  검증오차 : {val_loss}   검증정확도 : {val_accuray}')\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iter)\n",
        "print(f'테스트 오차 : {test_loss}   테스트 정학도 : {test_acc}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "building basic gru model ...\n",
            "에폭 : 1  검증오차 : 0.6935031635284424   검증정확도 : 50.63999938964844\n",
            "에폭 : 2  검증오차 : 0.6955553000450134   검증정확도 : 51.68000030517578\n",
            "에폭 : 3  검증오차 : 0.7093723979949951   검증정확도 : 49.65999984741211\n",
            "에폭 : 4  검증오차 : 0.5235813743114471   검증정확도 : 76.15999603271484\n",
            "에폭 : 5  검증오차 : 0.39257103457450865   검증정확도 : 83.31999969482422\n",
            "에폭 : 6  검증오차 : 0.31888863639831544   검증정확도 : 86.68000030517578\n",
            "에폭 : 7  검증오차 : 0.32119335041046143   검증정확도 : 87.05999755859375\n",
            "에폭 : 8  검증오차 : 0.3444256635665894   검증정확도 : 86.87999725341797\n",
            "에폭 : 9  검증오차 : 0.40229130001068114   검증정확도 : 86.83999633789062\n",
            "에폭 : 10  검증오차 : 0.41135893411636354   검증정확도 : 86.07999420166016\n",
            "에폭 : 11  검증오차 : 0.4544870388031006   검증정확도 : 86.83999633789062\n",
            "에폭 : 12  검증오차 : 0.39552955169677734   검증정확도 : 87.18000030517578\n",
            "에폭 : 13  검증오차 : 0.42498252353668214   검증정확도 : 86.75999450683594\n",
            "에폭 : 14  검증오차 : 0.46009779634475706   검증정확도 : 86.73999786376953\n",
            "에폭 : 15  검증오차 : 0.499126589012146   검증정확도 : 86.5999984741211\n",
            "에폭 : 16  검증오차 : 0.46410519771575925   검증정확도 : 87.22000122070312\n",
            "에폭 : 17  검증오차 : 0.4952790862083435   검증정확도 : 87.15999603271484\n",
            "에폭 : 18  검증오차 : 0.5114762350082398   검증정확도 : 87.29999542236328\n",
            "에폭 : 19  검증오차 : 0.5027324436187744   검증정확도 : 86.23999786376953\n",
            "에폭 : 20  검증오차 : 0.4849595097541809   검증정확도 : 87.15999603271484\n",
            "에폭 : 21  검증오차 : 0.4858650325775146   검증정확도 : 87.4000015258789\n",
            "에폭 : 22  검증오차 : 0.47506519355773924   검증정확도 : 87.36000061035156\n",
            "에폭 : 23  검증오차 : 0.5587655479431153   검증정확도 : 86.47999572753906\n",
            "에폭 : 24  검증오차 : 0.504974995803833   검증정확도 : 88.0199966430664\n",
            "에폭 : 25  검증오차 : 0.5664088656425476   검증정확도 : 87.97999572753906\n",
            "에폭 : 26  검증오차 : 0.553906705713272   검증정확도 : 88.0999984741211\n",
            "에폭 : 27  검증오차 : 0.5433662628173828   검증정확도 : 87.93999481201172\n",
            "에폭 : 28  검증오차 : 0.5626194012641906   검증정확도 : 88.07999420166016\n",
            "에폭 : 29  검증오차 : 0.57807639503479   검증정확도 : 88.19999694824219\n",
            "에폭 : 30  검증오차 : 0.5929033643245697   검증정확도 : 87.86000061035156\n",
            "에폭 : 31  검증오차 : 0.6126104120254516   검증정확도 : 87.9000015258789\n",
            "에폭 : 32  검증오차 : 0.6136295413255691   검증정확도 : 87.86000061035156\n",
            "에폭 : 33  검증오차 : 0.6353886204719543   검증정확도 : 87.79999542236328\n",
            "에폭 : 34  검증오차 : 0.5911645381450653   검증정확도 : 87.9000015258789\n",
            "에폭 : 35  검증오차 : 0.6409973664283752   검증정확도 : 87.6199951171875\n",
            "에폭 : 36  검증오차 : 0.5024785588264465   검증정확도 : 87.47999572753906\n",
            "에폭 : 37  검증오차 : 0.47864956874847414   검증정확도 : 87.68000030517578\n",
            "에폭 : 38  검증오차 : 0.47741446866989135   검증정확도 : 87.55999755859375\n",
            "에폭 : 39  검증오차 : 0.5417538932800293   검증정확도 : 87.69999694824219\n",
            "에폭 : 40  검증오차 : 0.5811686388015747   검증정확도 : 87.73999786376953\n",
            "테스트 오차 : 0.5659333158493042   테스트 정학도 : 86.4959945678711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gay0F66I48U5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}