{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-GRU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMuMzaGgrCI611ywsf+klag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heugyu/notebook/blob/master/CNN_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiYPxuQo8w35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu\n",
        "# CNN\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\" Initialize the model by setting up the layers\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # initial layer is resnet\n",
        "        self.resnet = model.resnet18(pretrained=True, progress=False)\n",
        "\n",
        "        # final fully connected layers\n",
        "        self.dense1 = nn.Linear(1000, 500)\n",
        "        self.dense2 = nn.Linear(500, 100)\n",
        "        self.dense3 = nn.Linear(100, 12)\n",
        "\n",
        "        # output layer\n",
        "        self.dense4 = nn.Linear(12, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" preform a forward pass of our model on some input and hidden state\"\"\"\n",
        "        x = self.resnet(x)\n",
        "\n",
        "        # apply three fully-connected Linear layers with ReLU activation function\n",
        "        x = self.dense1(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        x = self.dense2(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        x = self.dense3(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        # output is a size 1 Tensor\n",
        "        x = self.dense4(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJxM9wgU93u2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRU\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu\n",
        "import torchvision.models as models \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class GRUnet(nn.Module):\n",
        "    def __init__(self, num_features, num_rows, batch_size, hidden_size, num_layers):\n",
        "        \"\"\" Initialize the model by setting up the layers \"\"\"\n",
        "        \n",
        "        super(GRUnet, self).__init__()\n",
        "\n",
        "        # initialize information about model\n",
        "        self.num_features = num_features\n",
        "        self.num_rows = num_rows\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # RNN - GRU Layer\n",
        "        self.rmm = nn.GRU(\n",
        "            batch_first=True, \n",
        "            input_size=self.num_features,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers\n",
        "            )\n",
        "        \n",
        "        # dropout layer\n",
        "        # self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # 3 fully-connected hidden layers - with an output of dim 1\n",
        "        self.link_layer = nn.Linear(self.hidden_size, 1000)\n",
        "        self.dense1 = nn.Linear(1000, 500)\n",
        "        self.dense2 = nn.Linear(500, 100)\n",
        "        self.dense3 = nn.Linear(100, 12)\n",
        "\n",
        "        # output layer\n",
        "        self.dense4 = nn.Linear(12, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" perform a forward pass of our model on some input and hidden state \"\"\"\n",
        "        # GRU layer\n",
        "        x, self.hidden = self.rnn(x, self.hidden)\n",
        "\n",
        "        # detatch the hidden layer to prevent further backpropagating. i.e fix the vanishing gradient problem\n",
        "        self.hideen = self.hieen.detach().cuda()\n",
        "\n",
        "        # apply a Dropout layer\n",
        "        # x = self.dropout(x)\n",
        "\n",
        "        # pass through the link_layer\n",
        "        x = self.link_layer(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        # apply three fully-connected Linear layers with ReLU activation funtion\n",
        "        x = self.dense1(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        x = self.dense2(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        x = self.dense3(x)\n",
        "        x = relu(x)\n",
        "        \n",
        "        # output is a size 1 Tensor\n",
        "        x = self.dense4(x)\n",
        "        return x\n",
        "    \n",
        "    def init_hidden(self, batch_size, hidden_size):\n",
        "        \"\"\" Initializes hidden state \"\"\"\n",
        "\n",
        "        # creates initial hidden state for GRU of zeroes\n",
        "        hidden = torch.ones(self.num_layers, self.num_rows, hidden_size).cuda()\n",
        "        return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCSw9KL6WNlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRU - CNN\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu\n",
        "import torchvision.models as models \n",
        "\n",
        "class DRUCNN(nn.Module):\n",
        "    def __init__(self, num_features, num_rows, batch_size, hidden_size, num_layers):\n",
        "        \"\"\" Initialize the model by setting up the layers \"\"\"\n",
        "        super(GRUCNN, self).__init__()\n",
        "\n",
        "        # initialize gru and cnn - the full models \n",
        "\n",
        "        # gru model params \n",
        "        self.num_features = num_features\n",
        "        self.num_rows = num_rows\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = num_layers\n",
        "\n",
        "        # resnet model\n",
        "        self.cnn = models.resnet18(pretrained=True, progress=False)\n",
        "\n",
        "        # RNN -GRU model\n",
        "        self.rnn = nn.GRU(\n",
        "            batch_first=True, \n",
        "            input_size=self.num_features,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers\n",
        "            )\n",
        "        \n",
        "        # init GRU hidden layer\n",
        "        self.hidden = self.init_hidden(batch_size=self.batch_size, hidden_size=hidden_size)\n",
        "        self.gru_output = nn.Linear(self.hidden_size, 1000)\n",
        "\n",
        "        # final fully connected layers\n",
        "        self.dense1 = nn.Linear(1000, 500)\n",
        "        self.dense2 = nn.Linear(500, 100)\n",
        "        self.dense3 = nn.Linear(100, 12)\n",
        "\n",
        "        # output layer\n",
        "        self.dense4 = nn.Linear(12, 1)\n",
        "\n",
        "    def forward(self, m_input):\n",
        "        \"\"\" preform a forward pass of our model on some input and hidden state \"\"\"\n",
        "\n",
        "        # input is in a tuple (gru_input, cnn_input)\n",
        "        gru_input, cnn_input = m_input\n",
        "\n",
        "        # gru\n",
        "        gru_out, self.hidden = self.rnn(gru_inbput, self.hidden)\n",
        "        \n",
        "        # detatch the hidden layer to prevent further backpropagating. i.e. fix the vanisging gradient problem\n",
        "        self.hidden = self.hidden.detach().cuda()\n",
        "\n",
        "        # pass thriugh linear layer\n",
        "        gru_out = torch.squeeze(self.gru_output(gru_out))\n",
        "\n",
        "        # cnn\n",
        "        cnn_out = self.cnn(cnn_input)\n",
        "\n",
        "        # add the outputs of grunet and cnn\n",
        "        x = gru_out.add(cnn_out)\n",
        "\n",
        "        # feed throught final layers \n",
        "        \n",
        "        # apply three fully-connected Linear layers with ReLU activection function\n",
        "        x = self.dense1(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        x = self.dense2(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        x = self.dense3(x)\n",
        "        x = relu(x)\n",
        "\n",
        "        # output is a size 1 Tensor\n",
        "        x = self.dense4(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def init_hidden(self, batch_size, hidden_size):\n",
        "        \"\"\" Initializes hidden state \"\"\"\n",
        "\n",
        "        # create initial hidden state for GRU of zeroes\n",
        "        hidden = torch.ones(self.num_layers, self.batch_size, hidden_size).cuda()\n",
        "        return hidden\n",
        "\n",
        "    def load_cnn_weights(self, cnn):\n",
        "        cnn_params = cnn.named_parameters()\n",
        "        gru_cnn_params = dict(self.cnn.named_parameters())\n",
        "\n",
        "        for name, cnn_param in cnn_params:\n",
        "            if name in gru_cnn_params:\n",
        "                gru_cnn_params[name].data.copy_(cnn_param.data)\n",
        "\n",
        "    def load_gru_weights(self, gru):\n",
        "        gru_params = gru.named_parameters()\n",
        "        gru_cnn_params = dict(self.rnn.named_parameters())\n",
        "\n",
        "        for name, gru_param in gru_params():\n",
        "            if name in gru_cnn_params:\n",
        "                gru_cnn_params[name].data.copy_(gru_param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytrjDOdcZA-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DFDataset(Dataset):\n",
        "    \"\"\" dataset with inputs of DataFrame of OCHLV + tech_ihds, ourputs of normalized np arrays \"\"\"\n",
        "\n",
        "    def __init__(self, inputs, lavels):\n",
        "        self.inputs, self.labels = []. labels\n",
        "        for item in inputs:\n",
        "            self.inputs.append(DFDataset.format(item))\n",
        "        self.c = 1 # one label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.inputs[i], self.labels[i]\n",
        "    \n",
        "    @staticmethod\n",
        "    def df_to_arr(df):\n",
        "        return np.array(df)\n",
        "    \n",
        "    @staticmethod\n",
        "    def normalize_df(df, norm_func):\n",
        "        \"\"\" Normalize df with norm_func \"\"\"\n",
        "        # apply norm_func on columns\n",
        "        df = df.apply(norm_func, axis=0)\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def drop_time_col(df):\n",
        "        # drop time column\n",
        "        return df.drop('time', axis=1)       \n",
        "    \n",
        "    @staticmethod\n",
        "    def format(df):\n",
        "        \"\"\"Combine all processing steps from dataframe input to output for training\"\"\"\n",
        "        # normalize with minmax\n",
        "        x = DFDataset.normalize_df(df, minmaxnorm)\n",
        "        x = DFDataset.drop_time_col(x)\n",
        "        x = DFDataset.df_to_arr(x)\n",
        "        return x   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtoIWzvZlrIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}